{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concert Tweet Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necesary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import count, when, col\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the spark-NLP session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust show output format to pandas-like\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "# support converting pandas to spark\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first go, there were 25k rows of null - where the schema did not match the data. I decided to do some quick cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_seps(in_file, out_file, sep):\n",
    "    \"\"\"removes newline characters that come before the line reaches four segments(3 separators)\n",
    "    and combines \"middle sections\" with extra separators into a single segment by removing the separators.\n",
    "    \n",
    "    Args:\n",
    "        in_file: path to read file\n",
    "        out_file: path to write file\n",
    "        sep: separator/delimitor\n",
    "    \"\"\"\n",
    "    n_chunks = 4\n",
    "    \n",
    "    with open(in_file, 'r') as rf:\n",
    "        with open(out_file, 'w') as wf:\n",
    "            while True:\n",
    "                line = rf.readline()\n",
    "                \n",
    "                # if end of file\n",
    "                if line == '':\n",
    "                    break\n",
    "                    \n",
    "                # if line has less than n_sep, strip the newline and add the next line\n",
    "                if len(line.split(sep)) < n_chunks:\n",
    "                    line = line.strip('\\n')\n",
    "                    line += rf.readline()\n",
    "                \n",
    "                wf.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_extra_seps('../../data/test_set_tweets.txt',\n",
    "                      '../../data/test_set_tweets_clean.txt',\n",
    "                     '\\t')\n",
    "remove_extra_seps('../../data/training_set_tweets.txt',\n",
    "                      '../../data/training_set_tweets_clean.txt',\n",
    "                     '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the schema\n",
    "tweet_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"t_id\", StringType(), True),\n",
    "    StructField(\"t_text\", StringType(), True),\n",
    "    StructField(\"t_dt\", TimestampType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = spark.read.csv('../../data/test_set_tweets_clean.txt', \n",
    "                              sep='\\t',\n",
    "                              schema=tweet_schema,\n",
    "                              header=\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_training = spark.read.csv('../../data/training_set_tweets_clean.txt', \n",
    "                                 sep=\"\\t\", \n",
    "                                 schema=tweet_schema,\n",
    "                                 header='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is unlabeled for our task, these test/train splits are not particularly useful, but a vestige of the original data set and purpose. We'll combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_test.union(tweets_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future: Consider reading the data as a single column and then parsing. Compare outcome / number of tweets retrieved to that with the csv reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Info About the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+-------------------+\n",
      "| user_id|      t_id|              t_text|               t_dt|\n",
      "+--------+----------+--------------------+-------------------+\n",
      "|36287076|7277196841|@CHELLEYCHELLEZ w...|2010-01-01 13:52:38|\n",
      "|36287076|7276402546| @iDejaTia lol u are|2010-01-01 13:18:14|\n",
      "|36287076|7276054760|@iDejaTia same he...|2010-01-01 13:03:09|\n",
      "|36287076|7276049365| @iDejaTia same here|2010-01-01 13:02:56|\n",
      "|36287076|7274735472|@EmpressNRG Wah g...|2010-01-01 12:06:18|\n",
      "+--------+----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select('*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----+\n",
      "|user_id| t_id|t_text| t_dt|\n",
      "+-------+-----+------+-----+\n",
      "|  34555|34490| 34232|54671|\n",
      "+-------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select([count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "        tweets.columns]).show()\n",
    "\n",
    "# print(\"\"\"+-------+-----+------+-----+\n",
    "# |user_id| t_id|t_text| t_dt|\n",
    "# +-------+-----+------+-----+\n",
    "# |  33289|33179| 32631|53805|\n",
    "# +-------+-----+------+-----+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8884863"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()\n",
    "\n",
    "# print(8884863)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8850656\n"
     ]
    }
   ],
   "source": [
    "# tweets.distinct().count()\n",
    "print(8850656)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the time stamp can be parsed from the end of the tweet text for many of these \"null\" datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=25513575, t_id='10334442280', t_text='', t_dt=None),\n",
       " Row(user_id=None, t_id=None, t_text=None, t_dt=None),\n",
       " Row(user_id=25513575, t_id='10333612651', t_text='', t_dt=None),\n",
       " Row(user_id=None, t_id=None, t_text=None, t_dt=None),\n",
       " Row(user_id=None, t_id=None, t_text=None, t_dt=None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.filter(col('t_dt').isNull()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn('datetime', \n",
    "                           F.when(F.col('t_dt').isNull(), \n",
    "                                  F.to_date(F.substring('t_text', -19, 19)))\n",
    "                           .otherwise(F.col('t_dt'))\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn('t_text', \n",
    "                           F.when(F.col('t_dt').isNull(), \n",
    "                                  F.expr('substring(t_text, 1, length(t_text)-20)'))\n",
    "                           .otherwise(F.col('t_text'))\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn('t_dt', F.col('datetime')).drop('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet and reload\n",
    "# tweets.write.parquet('../../data/tweets.parquet')\n",
    "tweets = spark.read.parquet('../../data/tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----+\n",
      "|user_id| t_id|t_text| t_dt|\n",
      "+-------+-----+------+-----+\n",
      "|  34555|34490| 34232|54671|\n",
      "+-------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select([count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "        tweets.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8884863"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna(how='any', subset=['t_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=25513575, t_id='10334442280', t_text='', t_dt=None),\n",
       " Row(user_id=25513575, t_id='10333612651', t_text='', t_dt=None),\n",
       " Row(user_id=16198727, t_id='6899029209', t_text='This vid cracked me up! haha I w', t_dt=None),\n",
       " Row(user_id=20106865, t_id='10362030419', t_text=\"Ladies and gentlemen... come and join me.  It'\", t_dt=None),\n",
       " Row(user_id=20106865, t_id='10005503765', t_text='I am talking #Survivor RIGHT NOW in stickam', t_dt=None)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.filter(col('t_dt').isNull()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly I could do some more/better data engineering here, but for this exercise, I'm going to move on, dropping any records with null values or t_text with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.filter(~(tweets.t_text == \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8829912"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concert tweets - Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am deciding to focus on english tweets for now. (may add spanish, others in the future based on presence in the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up the pieces of my pipeline to extract text info from the tweets (we'll use a pretrained pipeline later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('t_text') \\\n",
    "     .setOutputCol('document')\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('token')\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['token']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemma')\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('clean_lemma') \\\n",
    "     .setCaseSensitive(False) \\\n",
    "     .setStopWords(eng_stopwords)\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['clean_lemma']) \\\n",
    "     .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           lemmatizer,\n",
    "           stopwords_cleaner,\n",
    "           finisher\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pipeline.fit(tweets).transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 't_id',\n",
       " 't_text',\n",
       " 't_dt',\n",
       " 'document',\n",
       " 'token',\n",
       " 'normalized',\n",
       " 'lemma',\n",
       " 'clean_lemma',\n",
       " 'finished_clean_lemma']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classifier: contains the word concert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "concert_tweets = tweets.withColumn('concert', F.array_contains('finished_clean_lemma', 'concert'))\n",
    "concert_tweets = concert_tweets.filter(concert_tweets['concert'] == 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(t_text=\"@herRoyalStarnes I just thought of the history broke down bmw's on bdays free concert tickets in the nose bleeds p (cont) http://tl.gd/4pp7k\"),\n",
       " Row(t_text='Y is me @RandiICandy, @EpitomeOfADiva, and Leila Bunny n here singing Mary J like we Mary J. We in concert yall buy a ticket yall.'),\n",
       " Row(t_text=\"@beccalexis sup Bee? How'd the shoot go? Will you be at the concert tonight?\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concert_tweets.select('t_text').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12477\n"
     ]
    }
   ],
   "source": [
    "# concert_tweets.count()\n",
    "\n",
    "print(12477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 't_id',\n",
       " 't_text',\n",
       " 't_dt',\n",
       " 'document',\n",
       " 'token',\n",
       " 'normalized',\n",
       " 'lemma',\n",
       " 'clean_lemma',\n",
       " 'finished_clean_lemma',\n",
       " 'concert']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concert_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|t_text                                                                                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@herRoyalStarnes I just thought of the history broke down bmw's on bdays free concert tickets in the nose bleeds p (cont) http://tl.gd/4pp7k|\n",
      "|Y is me @RandiICandy, @EpitomeOfADiva, and Leila Bunny n here singing Mary J like we Mary J. We in concert yall buy a ticket yall.          |\n",
      "|@beccalexis sup Bee? How'd the shoot go? Will you be at the concert tonight?                                                                |\n",
      "|RT @BoomKack: Janet was at Lady Gaga concert tonight she is everything!!!!!! Can't touch her!                                               |\n",
      "|Concert tonight at the bellyup! The grouch& mr fab                                                                                          |\n",
      "|They Played #FLEX @ The Jigga Concert... And #MrHitDatHoe                                                                                   |\n",
      "|My First Concert... Then I'm seeing one of the best to ever do it                                                                           |\n",
      "|In The Library With @NickAustinG... He Tryin To Get On His Jigga Shit... Oh We Goin To The Concert Tonite                                   |\n",
      "|@RockStarRenRen lol is we going to this concert                                                                                             |\n",
      "|Sooo go b4 u wet ur self  @ANGELicNES: Hhuuuuhh I wanna go to this Jayz @SongzYuuup and Jeezy concert sooo bad                              |\n",
      "|@Magpiez uhm who in their right mind would pass up a John mayer concert?!?                                                                  |\n",
      "|awwww :) RT @_tierra: How cute! RT @Djrayyadig: I'm even more excited that I'm at the concert w/ @allantemitchell                           |\n",
      "|@arinicolelife are u going to the drizzy concert up there??? (State)                                                                        |\n",
      "|@Britt_Garrison jus sent me some audio from the concert lmao thats why i love my bf :)                                                      |\n",
      "|RT @CoutureiCON: I hope that everyone is at the Wharton for the FREE concert! BJ the Chicago Kid, Dom Kennedy and Keely!!                   |\n",
      "|i wish i was goin to the concert at state :( an having crack chicken later                                                                  |\n",
      "|Needs a concert asap....sept 9th come soon please.....                                                                                      |\n",
      "|I'm sittin here listenin 2 *secret lovers* n it made me lmao becuz I bet that was a long quiet ride hm frm d concert 4 d couples            |\n",
      "|Bree's heading to maryland tonight for another jonny lang concert. I've got the kids! #fb http://myloc.me/1nQui                             |\n",
      "|Jonny Lang concert, anyone else here? http://pic.gd/693682                                                                                  |\n",
      "|80s Babies Concert at Nokia Theater http://bit.ly/4wXmmO                                                                                    |\n",
      "|Bill Cosby - Cosby Joins Hancocks Birthday Concert Celebrations http://is.gd/9qlkh                                                          |\n",
      "|Whitney Houston - More Concert Woes For Houston http://is.gd/9a5zX                                                                          |\n",
      "|Whitney Houston - Houston Sparks New Health Concerns At Australia Concert http://is.gd/91Cs5                                                |\n",
      "|Jason Derulo- Helps @ BET's \"SOS: Help for Haiti\" Concert & Telethon http://is.gd/8GB7i                                                     |\n",
      "|Whitney Houston - Houston Scraps New Zealand Concert http://is.gd/8yxA3                                                                     |\n",
      "|@Ms_CPerry Concert tickets are like 150 but for the VIP is like 220-ish.. I fly southwest and plane ticket is like 250-ish...               |\n",
      "|those who had concerts... Yall know me better than that... #c'monson hahaha  http://myloc.me/1GDH7                                          |\n",
      "|RT @QueenzChicka88: Finally Off!!!Gotta get home so I can get ready to see my Boii\"HOVVVVVV\"2nite!!!:) -- U go to alllll his concerts! Lol  |\n",
      "|@juliembaby julz u going to the hov concert??                                                                                               |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concert_tweets.select(\"t_text\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classifier: contains the word concert or similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "concert_plus = tweets.withColumn('concert', F.array_contains('finished_clean_lemma', 'concert'))\\\n",
    "                     .withColumn('tour', F.array_contains('finished_clean_lemma', 'tour'))\\\n",
    "                     .withColumn('gig', F.array_contains('finished_clean_lemma', 'gig'))\\\n",
    "                     .withColumn('show', F.array_contains('finished_clean_lemma', 'show'))\n",
    "concert_plus = concert_plus.withColumn('concert_like', col('concert')|col('tour')|col('gig'))\n",
    "concert_plus = concert_plus.filter(concert_plus.concert_like == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|t_text                                                                                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@Lauralu2u yeps I had curve than the tour.   Love my Droid                                                                                  |\n",
      "|@herRoyalStarnes I just thought of the history broke down bmw's on bdays free concert tickets in the nose bleeds p (cont) http://tl.gd/4pp7k|\n",
      "|Y is me @RandiICandy, @EpitomeOfADiva, and Leila Bunny n here singing Mary J like we Mary J. We in concert yall buy a ticket yall.          |\n",
      "|@joeymcintyre You've got to be a LITTLE bit silly on tour or you wouldn't be YOU! ;)                                                        |\n",
      "|Wrapped Product Development on samples, now headed to FedEx to snd to my client! Have fun on tour babe! Hope u like em! Wish I could twitpic|\n",
      "|@beccalexis sup Bee? How'd the shoot go? Will you be at the concert tonight?                                                                |\n",
      "|RT @BoomKack: Janet was at Lady Gaga concert tonight she is everything!!!!!! Can't touch her!                                               |\n",
      "|Up and at em..got booked for anotha gig in 10 days.. 3-5 Models, 3 looks a piece..need a intern/assistant BAD                               |\n",
      "|RT @DerrickSwerve: I Cant Wait 4 @Hollywood2BK and @HoffaBillz To Have These Hip Hop Groupies So I Can Smash All The Leftover Scraps On Tour|\n",
      "|Watching \"Shades of Brooklyn\" waiting for this pizza... #shoutouts to @HeartbreakHolly for his HBO gig, big bizne$$!!! Salute!              |\n",
      "|Concert tonight at the bellyup! The grouch& mr fab                                                                                          |\n",
      "|' My girl love me but fuck it my heart beats slow & right now the tour bus is lookin like a freak show '                                    |\n",
      "|@JCanMakeuFamous I'm wit it.. Paying gig?                                                                                                   |\n",
      "|@JaeBarz aww dnt trip.. Hardwork pays off.. You'll get ur gig.. Be ez on urself                                                             |\n",
      "|They Played #FLEX @ The Jigga Concert... And #MrHitDatHoe                                                                                   |\n",
      "|My First Concert... Then I'm seeing one of the best to ever do it                                                                           |\n",
      "|In The Library With @NickAustinG... He Tryin To Get On His Jigga Shit... Oh We Goin To The Concert Tonite                                   |\n",
      "|@RockStarRenRen lol is we going to this concert                                                                                             |\n",
      "|Sooo go b4 u wet ur self  @ANGELicNES: Hhuuuuhh I wanna go to this Jayz @SongzYuuup and Jeezy concert sooo bad                              |\n",
      "|@Magpiez uhm who in their right mind would pass up a John mayer concert?!?                                                                  |\n",
      "|awwww :) RT @_tierra: How cute! RT @Djrayyadig: I'm even more excited that I'm at the concert w/ @allantemitchell                           |\n",
      "|@arinicolelife are u going to the drizzy concert up there??? (State)                                                                        |\n",
      "|@Britt_Garrison jus sent me some audio from the concert lmao thats why i love my bf :)                                                      |\n",
      "|RT @CoutureiCON: I hope that everyone is at the Wharton for the FREE concert! BJ the Chicago Kid, Dom Kennedy and Keely!!                   |\n",
      "|i wish i was goin to the concert at state :( an having crack chicken later                                                                  |\n",
      "|You know you have a lot of music when you fill a 80gig external....                                                                         |\n",
      "|Needs a concert asap....sept 9th come soon please.....                                                                                      |\n",
      "|Nerves starting to set in..... Auditions for turning stone gig tomorrow....                                                                 |\n",
      "|Goooooh-jus :D had a fantastic day... Tour continuous tomorrow :) http://tweetphoto.com/13837678                                            |\n",
      "|@melmah  Hey, give me a shout.  I got a potential gig for you.                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concert_plus.select(\"t_text\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this super small sample, it doesn't seem like these alternate words are adding a lot to our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future: maybe combination of show/tour/gig and musician/group name in addition to the concert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have labeled data, and I'm not sure the best technique for clustering text data in this situation. Or how we would evaluate which techniqes are doing the best job identifying our concert tweets, and whether they are worth the extra complexity/computational requirements.\n",
    "\n",
    "For now, I'm going to move on using the \"concert\" lemma classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concert_tweets.select('user_id', 't_text', 't_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>t_text</th><th>t_dt</th></tr>\n",
       "<tr><td>85691996</td><td>@herRoyalStarnes ...</td><td>2010-01-22 10:17:15</td></tr>\n",
       "<tr><td>85691996</td><td>Y is me @RandiICa...</td><td>2010-01-15 16:22:28</td></tr>\n",
       "<tr><td>25611870</td><td>@beccalexis sup B...</td><td>2010-01-30 00:00:00</td></tr>\n",
       "<tr><td>25611870</td><td>RT @BoomKack: Jan...</td><td>2010-01-24 00:00:00</td></tr>\n",
       "<tr><td>30387809</td><td>Concert tonight a...</td><td>2009-12-09 15:06:12</td></tr>\n",
       "<tr><td>71702459</td><td>They Played #FLEX...</td><td>2010-02-22 19:59:44</td></tr>\n",
       "<tr><td>71702459</td><td>My First Concert....</td><td>2010-02-22 19:26:40</td></tr>\n",
       "<tr><td>71702459</td><td>In The Library Wi...</td><td>2010-02-22 11:32:56</td></tr>\n",
       "<tr><td>49483366</td><td>@RockStarRenRen l...</td><td>2009-07-30 11:56:06</td></tr>\n",
       "<tr><td>28528232</td><td>Sooo go b4 u wet ...</td><td>2010-01-13 16:37:44</td></tr>\n",
       "<tr><td>22542268</td><td>@Magpiez uhm who ...</td><td>2010-03-02 01:47:55</td></tr>\n",
       "<tr><td>27446890</td><td>awwww :) RT @_tie...</td><td>2010-03-14 18:24:28</td></tr>\n",
       "<tr><td>27446890</td><td>@arinicolelife ar...</td><td>2010-03-02 09:21:26</td></tr>\n",
       "<tr><td>27446890</td><td>@Britt_Garrison j...</td><td>2010-02-11 19:58:51</td></tr>\n",
       "<tr><td>27446890</td><td>RT @CoutureiCON: ...</td><td>2010-02-11 17:45:41</td></tr>\n",
       "<tr><td>27446890</td><td>i wish i was goin...</td><td>2010-02-11 16:21:15</td></tr>\n",
       "<tr><td>22408811</td><td>Needs a concert a...</td><td>2009-08-18 00:00:00</td></tr>\n",
       "<tr><td>16608886</td><td>I&#x27;m sittin here l...</td><td>2010-03-02 10:55:52</td></tr>\n",
       "<tr><td>5713172</td><td>Bree&#x27;s heading to...</td><td>2009-11-07 10:49:57</td></tr>\n",
       "<tr><td>5713172</td><td>Jonny Lang concer...</td><td>2009-11-05 18:48:41</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+-------------------+\n",
       "| user_id|              t_text|               t_dt|\n",
       "+--------+--------------------+-------------------+\n",
       "|85691996|@herRoyalStarnes ...|2010-01-22 10:17:15|\n",
       "|85691996|Y is me @RandiICa...|2010-01-15 16:22:28|\n",
       "|25611870|@beccalexis sup B...|2010-01-30 00:00:00|\n",
       "|25611870|RT @BoomKack: Jan...|2010-01-24 00:00:00|\n",
       "|30387809|Concert tonight a...|2009-12-09 15:06:12|\n",
       "|71702459|They Played #FLEX...|2010-02-22 19:59:44|\n",
       "|71702459|My First Concert....|2010-02-22 19:26:40|\n",
       "|71702459|In The Library Wi...|2010-02-22 11:32:56|\n",
       "|49483366|@RockStarRenRen l...|2009-07-30 11:56:06|\n",
       "|28528232|Sooo go b4 u wet ...|2010-01-13 16:37:44|\n",
       "|22542268|@Magpiez uhm who ...|2010-03-02 01:47:55|\n",
       "|27446890|awwww :) RT @_tie...|2010-03-14 18:24:28|\n",
       "|27446890|@arinicolelife ar...|2010-03-02 09:21:26|\n",
       "|27446890|@Britt_Garrison j...|2010-02-11 19:58:51|\n",
       "|27446890|RT @CoutureiCON: ...|2010-02-11 17:45:41|\n",
       "|27446890|i wish i was goin...|2010-02-11 16:21:15|\n",
       "|22408811|Needs a concert a...|2009-08-18 00:00:00|\n",
       "|16608886|I'm sittin here l...|2010-03-02 10:55:52|\n",
       "| 5713172|Bree's heading to...|2009-11-07 10:49:57|\n",
       "| 5713172|Jonny Lang concer...|2009-11-05 18:48:41|\n",
       "+--------+--------------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12444\n"
     ]
    }
   ],
   "source": [
    "# df.count()\n",
    "print(12444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------------+\n",
      "| user_id|              t_text|               t_dt|\n",
      "+--------+--------------------+-------------------+\n",
      "|85691996|@herRoyalStarnes ...|2010-01-22 10:17:15|\n",
      "+--------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(withReplacement=None, fraction=0.01, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.withColumnRenamed('t_text', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id', 'text', 't_dt']"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=71702459, text='In The Library With @NickAustinG... He Tryin To Get On His Jigga Shit... Oh We Goin To The Concert Tonite', t_dt=datetime.datetime(2010, 2, 22, 11, 32, 56)),\n",
       " Row(user_id=49483366, text='@RockStarRenRen lol is we going to this concert', t_dt=datetime.datetime(2009, 7, 30, 11, 56, 6)),\n",
       " Row(user_id=19688989, text='@bizymare   My son lives in Kenosha, I\"m down there about a dozen times a year.    The concert was for the Kenosha area home schoolers.', t_dt=datetime.datetime(2009, 12, 11, 22, 16, 24)),\n",
       " Row(user_id=20019157, text='@itSHOWTIME how was the concert?', t_dt=datetime.datetime(2010, 1, 19, 0, 0)),\n",
       " Row(user_id=49477598, text=\"@audiobebop what time? I'm suppose to go to a concert tonight with Juda. uhh\", t_dt=datetime.datetime(2010, 1, 15, 9, 26, 7)),\n",
       " Row(user_id=33814590, text='@MizzDania How was the concert?', t_dt=datetime.datetime(2010, 1, 22, 0, 0)),\n",
       " Row(user_id=29299184, text='The A was poppin @S_C_ @SongzYuuup and jeezy held the concert down.. Back in tally 2 papers and 2 midterms due tues. # backtoboredom', t_dt=datetime.datetime(2010, 2, 28, 17, 31, 30)),\n",
       " Row(user_id=60822006, text='@melodyxxx LOL u ladies go hard! Are u guys going the kid cudi concert on Saturday?', t_dt=datetime.datetime(2009, 11, 23, 10, 5, 41)),\n",
       " Row(user_id=27474555, text=\"Look, I'm sorry about Chile and Haiti, but for real, do we need another benefit concert? COME ON! What about people here, in the USA?\", t_dt=datetime.datetime(2010, 2, 27, 8, 56, 55)),\n",
       " Row(user_id=24024339, text='U r!  had to get Fred cause he was doing a Radio One concert yesterday!  I start in Nov...and u know I got u!  In fact I want PAJAM to d ...', t_dt=datetime.datetime(2009, 10, 11, 7, 23, 33)),\n",
       " Row(user_id=14529852, text='Gearing up for The Decemberists concert tonight by listening to all of their music. T minus 6 hours until the doors open.', t_dt=datetime.datetime(2009, 8, 11, 12, 38, 36)),\n",
       " Row(user_id=44382895, text='Getting ready to leave for a concert of an old friends new boyfriend. Should be interesting!', t_dt=datetime.datetime(2010, 1, 16, 19, 20, 49)),\n",
       " Row(user_id=23347429, text=\"@DustinLuminate hey, when ya'll do concerts do u take your own lighting & sound with you?\", t_dt=datetime.datetime(2009, 8, 19, 22, 49, 53)),\n",
       " Row(user_id=23347429, text='Listening to some @JonasBrothers + getting ready for their concert.....19 more days!!!!!*******', t_dt=datetime.datetime(2009, 7, 10, 22, 58, 16)),\n",
       " Row(user_id=30614011, text=\"@chrisettefan No she's the headliner. The concert started @ 8 Lem was leaving the stage @ 8:14. The girl had 30mins. Chrissy rocked the...\", t_dt=datetime.datetime(2010, 3, 4, 7, 19, 50)),\n",
       " Row(user_id=38593112, text=\"@RecruitZero CONGRATS!! ..And, I'll have you know sir that concerts and nightspots provide wonderful networking opps. ::grr::\", t_dt=datetime.datetime(2009, 9, 12, 19, 7, 7)),\n",
       " Row(user_id=19325588, text='RT @Platinumstroke: @ibhappy  Plz RT! Pain2Power Foundation Benefit & Inspirational Concert in memory of (cont) http://tl.gd/8f6qv', t_dt=datetime.datetime(2010, 2, 11, 16, 34, 57)),\n",
       " Row(user_id=26809809, text='RT @LiveNation: 500th RT wins $100 CONCERT CASH! http://bit.ly/9o9vZt', t_dt=datetime.datetime(2010, 2, 19, 13, 55, 56)),\n",
       " Row(user_id=26809809, text=\"I can't wait to see NKOTB in concert @LiveNation with my $100 concert cash!!! http://bit.ly/cT4csz\", t_dt=datetime.datetime(2010, 1, 28, 14, 19, 8)),\n",
       " Row(user_id=26809809, text=\"I can't wait to see NKOTB in concert @LiveNation with my $100 concert cash!!http://bit.ly/cT4csz\", t_dt=datetime.datetime(2010, 1, 28, 14, 18, 30))]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who\n",
    "\n",
    "For the sake of time, I focused on pop and hip hop artists from 2009/2010 (data from wikipedia). This is extra tricky when tweeters use the artist handles (eg @JonasBrothers), again this is an area for future iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import artist list\n",
    "with open('../../data/musicians.txt', 'r') as f:\n",
    "     artists = f.read().splitlines()\n",
    "        \n",
    "artists = list(set(artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_dl download started this may take some time.\n",
      "Approx size to download 167.3 MB\n",
      "[OK!]\n",
      "+--------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| user_id|                text|               t_dt|            document|            sentence|               token|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|\n",
      "+--------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|71702459|In The Library Wi...|2010-02-22 11:32:56|[[document, 0, 10...|[[document, 0, 34...|[[token, 0, 1, In...|[[token, 0, 1, In...|[[token, 0, 1, In...|[[token, 0, 1, in...|[[pos, 0, 1, IN, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 31, T...|\n",
      "+--------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"explain_document_dl\", lang=\"en\")\n",
    "\n",
    "annotation = pipeline.transform(sample)\n",
    "\n",
    "annotation.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|result                                                 |\n",
      "+-------------------------------------------------------+\n",
      "|[The Library With @NickAustinG, The Concert Tonite]    |\n",
      "|[@RockStarRenRen]                                      |\n",
      "|[Kenosha, I\"m, Kenosha]                                |\n",
      "|[]                                                     |\n",
      "|[Juda]                                                 |\n",
      "|[@MizzDania]                                           |\n",
      "|[@S_C_ @SongzYuuup]                                    |\n",
      "|[]                                                     |\n",
      "|[I'm, Chile, Haiti, USA]                               |\n",
      "|[Fred, Radio One, PAJAM]                               |\n",
      "|[Decemberists]                                         |\n",
      "|[]                                                     |\n",
      "|[&]                                                    |\n",
      "|[@JonasBrothers +, !!!!!*******]                       |\n",
      "|[Lem, Chrissy]                                         |\n",
      "|[@RecruitZero, I'll]                                   |\n",
      "|[Pain2Power Foundation Benefit & Inspirational Concert]|\n",
      "|[]                                                     |\n",
      "|[NKOTB]                                                |\n",
      "|[NKOTB]                                                |\n",
      "+-------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotation.select(\"entities.result\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uncle Kracker',\n",
       " 'Warren G',\n",
       " 'Pitbull',\n",
       " 'Sa-Ra Creative Partners',\n",
       " 'Linkin Park',\n",
       " 'Grandmaster Flash',\n",
       " 'Asher Roth',\n",
       " 'Paul Wall',\n",
       " 'Kelly Clarkson',\n",
       " \"Ol' Dirty Bastard\",\n",
       " 'DJ Drama',\n",
       " 'Tyga',\n",
       " 'Busta Rhymes',\n",
       " 'Big Scoob',\n",
       " 'Rivers Cuomo',\n",
       " 'The Alchemist',\n",
       " 'Clipse',\n",
       " 'Romeo',\n",
       " 'Bow Wow',\n",
       " 'Hurricane Chris',\n",
       " 'Shinedown',\n",
       " 'Kurupt',\n",
       " 'T-Pain',\n",
       " 'Juvenile',\n",
       " \"Cam'ron\",\n",
       " 'Freeway',\n",
       " 'The Band Perry',\n",
       " 'Stoupe the Enemy of Mankind',\n",
       " 'Dorrough',\n",
       " 'Sammie',\n",
       " 'Slim Thug',\n",
       " 'CRUNK23',\n",
       " 'Classified',\n",
       " 'Eyedea',\n",
       " 'Jordin Sparks',\n",
       " 'Timbaland',\n",
       " 'The Sounds',\n",
       " 'La Coka Nostra',\n",
       " 'Bruno Mars',\n",
       " 'Sean Kingston',\n",
       " 'Juicy J',\n",
       " 'Adam Lambert',\n",
       " 'Playaz Circle',\n",
       " 'Skull Gang',\n",
       " 'JAY Z',\n",
       " 'Soap Nation',\n",
       " 'Cobra Starship',\n",
       " 'Gorilla Zoe',\n",
       " 'Lady Antebellum',\n",
       " 'DJ Green Lantern',\n",
       " 'Chipmunk',\n",
       " 'Kid Cudi',\n",
       " 'Young Jeezy',\n",
       " 'k-os',\n",
       " 'Crunk Chris',\n",
       " 'Travie McCoy',\n",
       " 'KRS-One & Buckshot',\n",
       " 'N.O.R.E.',\n",
       " 'Decemberists',\n",
       " 'Kris Allen',\n",
       " 'Dead Prez',\n",
       " 'Maino',\n",
       " 'Chico DeBarge',\n",
       " 'Noah23 & Madadam',\n",
       " 'DS',\n",
       " 'Mims',\n",
       " 'B.G.',\n",
       " 'T.I.',\n",
       " 'Daughtry',\n",
       " 'Tede',\n",
       " 'DOOM',\n",
       " 'Jamie Foxx',\n",
       " 'Mariah Carey',\n",
       " 'Orianthi',\n",
       " 'Lisa \"Left Eye\" Lopes',\n",
       " 'Twista',\n",
       " 'Leighton Meester',\n",
       " 'Beanie Sigel',\n",
       " 'Pink',\n",
       " 'Zion I',\n",
       " 'Insane Clown Posse',\n",
       " 'Brother Ali',\n",
       " 'Pastor Troy',\n",
       " 'Street Sweeper Social Club',\n",
       " 'Ciara',\n",
       " 'New Boyz',\n",
       " 'Young Money',\n",
       " 'Shontelle',\n",
       " 'OJ Da Juiceman',\n",
       " 'Esoteric',\n",
       " 'Stevie Stone',\n",
       " 'T-Pain, Ludacris',\n",
       " 'Lady Gaga',\n",
       " 'Violent J',\n",
       " 'Cali Swag District',\n",
       " 'Hussein Fatal',\n",
       " 'Wynter',\n",
       " 'B.o.B',\n",
       " 'Busdriver',\n",
       " 'Lil Wyte',\n",
       " 'Danny!',\n",
       " 'Rick Ross',\n",
       " 'Mike Posner',\n",
       " 'Jerrod Niemann',\n",
       " 'O.S.T.R.',\n",
       " 'Trick Daddy',\n",
       " 'DJ Khaled',\n",
       " 'Dr. Dre',\n",
       " 'The Cataracs',\n",
       " 'Mike Epps',\n",
       " 'Jeremih',\n",
       " 'Obie Trice',\n",
       " 'Plies',\n",
       " 'Jason Derulo',\n",
       " '50 Cent',\n",
       " 'Redman',\n",
       " 'Michael Bubl√©',\n",
       " 'Wale',\n",
       " 'Snoop Dogg',\n",
       " 'Miley Cyrus',\n",
       " 'Glee Cast',\n",
       " 'Mos Def',\n",
       " 'Nelly',\n",
       " 'Mike Jones',\n",
       " 'Akon',\n",
       " 'Madadam',\n",
       " 'Birdman',\n",
       " 'Sugarland',\n",
       " 'Taylor Swift',\n",
       " 'Famous Playaz',\n",
       " 'Dev',\n",
       " 'Eminem featuring Rihanna',\n",
       " 'AZ',\n",
       " 'Lord Infamous, T-Rock & II Tone',\n",
       " 'Noah23',\n",
       " 'Lil Wayne',\n",
       " 'Soulja Boy',\n",
       " 'Kesha',\n",
       " 'Webstar',\n",
       " 'Selena Gomez & the Scene',\n",
       " 'D-Block',\n",
       " 'Kottonmouth Kings',\n",
       " 'Felt',\n",
       " 'Kanye West',\n",
       " 'Flo Rida',\n",
       " 'Usher',\n",
       " 'Gudda Gudda',\n",
       " 'Hannah Montana',\n",
       " 'OneRepublic',\n",
       " 'Grand Puba',\n",
       " '3OH!3',\n",
       " 'Owl City',\n",
       " 'Enrique Iglesias',\n",
       " 'Plague Language',\n",
       " 'Method Man',\n",
       " 'Barenaked Ladies',\n",
       " 'Fabolous',\n",
       " 'Hell Rell',\n",
       " 'Swollen Members',\n",
       " 'Lord Kufu',\n",
       " 'Keri Hilson',\n",
       " 'Krizz Kaliko',\n",
       " 'Trey Songz',\n",
       " 'Young Money Entertainment',\n",
       " 'The Fray',\n",
       " 'Eminem',\n",
       " 'Raekwon',\n",
       " 'Jim Jones',\n",
       " 'Justin Bieber',\n",
       " 'Project Pat',\n",
       " 'Fast Life Yungstaz',\n",
       " 'Justin Timberlake',\n",
       " 'Beyonc√©',\n",
       " 'DJ Paul',\n",
       " 'Gucci Mane',\n",
       " 'Miranda Lambert',\n",
       " 'The Roots',\n",
       " 'L.E.G.A.C.Y.',\n",
       " 'Britney Spears',\n",
       " 'DJ Quik',\n",
       " 'B-Real',\n",
       " 'UGK',\n",
       " 'Hopsin',\n",
       " 'Willy Northpole',\n",
       " 'Abilities',\n",
       " 'Rihanna',\n",
       " 'Taio Cruz',\n",
       " 'Katy Perry',\n",
       " 'Far East Movement',\n",
       " 'will.i.am',\n",
       " 'BlakRoc',\n",
       " 'Lil Jon',\n",
       " 'Jason Mraz',\n",
       " 'Lloyd',\n",
       " \"Lil' Flip\",\n",
       " 'Tech N9ne',\n",
       " 'Hayley Williams',\n",
       " 'Jadakiss',\n",
       " 'Jay-Z',\n",
       " 'Jeezy',\n",
       " 'Killer Mike',\n",
       " 'Lil Boosie',\n",
       " 'Boys Like Girls',\n",
       " 'NKOTB',\n",
       " 'Chris Brown',\n",
       " 'Randy Travis',\n",
       " 'Guru',\n",
       " 'Ace Hood',\n",
       " 'MF DOOM',\n",
       " 'Yukmouth',\n",
       " 'Alicia Keys',\n",
       " 'Baracuda',\n",
       " 'Q-Tip',\n",
       " 'Joe Budden',\n",
       " 'Maroon 5',\n",
       " 'Livestock',\n",
       " 'Nicki Minaj',\n",
       " 'Sheek Louch',\n",
       " 'Capone-N-Noreaga',\n",
       " 'Ray J',\n",
       " 'Method Man & Redman',\n",
       " 'Ne-Yo',\n",
       " 'La Roux',\n",
       " 'Sean Paul',\n",
       " 'Neon Trees',\n",
       " \"Triple C's\",\n",
       " 'Kings of Leon',\n",
       " 'Del the Funky Homosapien',\n",
       " 'Iyaz',\n",
       " 'Fat Joe',\n",
       " 'The Black Eyed Peas',\n",
       " 'Sara Bareilles',\n",
       " 'Dizzee Rascal',\n",
       " 'Kevin McCall',\n",
       " 'Havoc',\n",
       " 'Blaq Poet',\n",
       " 'Mack 10',\n",
       " 'David Guetta',\n",
       " 'J Dilla',\n",
       " 'Royce da 5\\'9\"',\n",
       " 'Haystak',\n",
       " 'Slaughterhouse',\n",
       " 'Drake',\n",
       " 'Kevin Rudolf',\n",
       " 'Rakim',\n",
       " 'Mr Hudson',\n",
       " 'Artists for Haiti',\n",
       " 'Skyzoo',\n",
       " 'Twiztid',\n",
       " 'U-God',\n",
       " 'The Script',\n",
       " 'Ghostface Killah',\n",
       " 'X-Raided',\n",
       " 'Soul Assassins',\n",
       " 'Jay Sean',\n",
       " 'Paramore',\n",
       " 'The All-American Rejects',\n",
       " 'The Main',\n",
       " 'Souls of Mischief',\n",
       " 'Carrie Underwood',\n",
       " 'Train',\n",
       " 'Ludacris']"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ugh. pyspark.sql.functions.typedLit doesn't exist yet in pyspark to pass the artists to a udf. \n",
    "# So I'm going to switch to pandas for this step\n",
    "\n",
    "entities_pd = annotation.select('entities.result', 'text').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_pd['who'] = [[entity for entity in e_list if entity in artists] for e_list in entities_pd['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>text</th>\n",
       "      <th>who</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[The Library With @NickAustinG, The Concert To...</td>\n",
       "      <td>In The Library With @NickAustinG... He Tryin T...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[@RockStarRenRen]</td>\n",
       "      <td>@RockStarRenRen lol is we going to this concert</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[Kenosha, I\"m, Kenosha]</td>\n",
       "      <td>@bizymare   My son lives in Kenosha, I\"m down ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>@itSHOWTIME how was the concert?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[Juda]</td>\n",
       "      <td>@audiobebop what time? I'm suppose to go to a ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[@MizzDania]</td>\n",
       "      <td>@MizzDania How was the concert?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[@S_C_ @SongzYuuup]</td>\n",
       "      <td>The A was poppin @S_C_ @SongzYuuup and jeezy h...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>@melodyxxx LOL u ladies go hard! Are u guys go...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[I'm, Chile, Haiti, USA]</td>\n",
       "      <td>Look, I'm sorry about Chile and Haiti, but for...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[Fred, Radio One, PAJAM]</td>\n",
       "      <td>U r!  had to get Fred cause he was doing a Rad...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>[Decemberists]</td>\n",
       "      <td>Gearing up for The Decemberists concert tonigh...</td>\n",
       "      <td>[Decemberists]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>Getting ready to leave for a concert of an old...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[&amp;]</td>\n",
       "      <td>@DustinLuminate hey, when ya'll do concerts do...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>[@JonasBrothers +, !!!!!*******]</td>\n",
       "      <td>Listening to some @JonasBrothers + getting rea...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>[Lem, Chrissy]</td>\n",
       "      <td>@chrisettefan No she's the headliner. The conc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>[@RecruitZero, I'll]</td>\n",
       "      <td>@RecruitZero CONGRATS!! ..And, I'll have you k...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>[Pain2Power Foundation Benefit &amp; Inspirational...</td>\n",
       "      <td>RT @Platinumstroke: @ibhappy  Plz RT! Pain2Pow...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @LiveNation: 500th RT wins $100 CONCERT CAS...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>[NKOTB]</td>\n",
       "      <td>I can't wait to see NKOTB in concert @LiveNati...</td>\n",
       "      <td>[NKOTB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>[NKOTB]</td>\n",
       "      <td>I can't wait to see NKOTB in concert @LiveNati...</td>\n",
       "      <td>[NKOTB]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               result  \\\n",
       "0   [The Library With @NickAustinG, The Concert To...   \n",
       "1                                   [@RockStarRenRen]   \n",
       "2                             [Kenosha, I\"m, Kenosha]   \n",
       "3                                                  []   \n",
       "4                                              [Juda]   \n",
       "5                                        [@MizzDania]   \n",
       "6                                 [@S_C_ @SongzYuuup]   \n",
       "7                                                  []   \n",
       "8                            [I'm, Chile, Haiti, USA]   \n",
       "9                            [Fred, Radio One, PAJAM]   \n",
       "10                                     [Decemberists]   \n",
       "11                                                 []   \n",
       "12                                                [&]   \n",
       "13                   [@JonasBrothers +, !!!!!*******]   \n",
       "14                                     [Lem, Chrissy]   \n",
       "15                               [@RecruitZero, I'll]   \n",
       "16  [Pain2Power Foundation Benefit & Inspirational...   \n",
       "17                                                 []   \n",
       "18                                            [NKOTB]   \n",
       "19                                            [NKOTB]   \n",
       "\n",
       "                                                 text             who  \n",
       "0   In The Library With @NickAustinG... He Tryin T...              []  \n",
       "1     @RockStarRenRen lol is we going to this concert              []  \n",
       "2   @bizymare   My son lives in Kenosha, I\"m down ...              []  \n",
       "3                    @itSHOWTIME how was the concert?              []  \n",
       "4   @audiobebop what time? I'm suppose to go to a ...              []  \n",
       "5                     @MizzDania How was the concert?              []  \n",
       "6   The A was poppin @S_C_ @SongzYuuup and jeezy h...              []  \n",
       "7   @melodyxxx LOL u ladies go hard! Are u guys go...              []  \n",
       "8   Look, I'm sorry about Chile and Haiti, but for...              []  \n",
       "9   U r!  had to get Fred cause he was doing a Rad...              []  \n",
       "10  Gearing up for The Decemberists concert tonigh...  [Decemberists]  \n",
       "11  Getting ready to leave for a concert of an old...              []  \n",
       "12  @DustinLuminate hey, when ya'll do concerts do...              []  \n",
       "13  Listening to some @JonasBrothers + getting rea...              []  \n",
       "14  @chrisettefan No she's the headliner. The conc...              []  \n",
       "15  @RecruitZero CONGRATS!! ..And, I'll have you k...              []  \n",
       "16  RT @Platinumstroke: @ibhappy  Plz RT! Pain2Pow...              []  \n",
       "17  RT @LiveNation: 500th RT wins $100 CONCERT CAS...              []  \n",
       "18  I can't wait to see NKOTB in concert @LiveNati...         [NKOTB]  \n",
       "19  I can't wait to see NKOTB in concert @LiveNati...         [NKOTB]  "
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_schema = StructType([\n",
    "                StructField(\"result\", ArrayType(StringType()), True),\n",
    "                StructField(\"text\", StringType(), True),\n",
    "                StructField(\"who\", ArrayType(StringType()), True)\n",
    "                ])\n",
    "\n",
    "who = spark.createDataFrame(entities_pd, schema=who_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['The Library With @NickAustinG', 'The Concert Tonite'], text='In The Library With @NickAustinG... He Tryin To Get On His Jigga Shit... Oh We Goin To The Concert Tonite', who=[]),\n",
       " Row(result=['@RockStarRenRen'], text='@RockStarRenRen lol is we going to this concert', who=[]),\n",
       " Row(result=['Kenosha', 'I\"m', 'Kenosha'], text='@bizymare   My son lives in Kenosha, I\"m down there about a dozen times a year.    The concert was for the Kenosha area home schoolers.', who=[]),\n",
       " Row(result=[], text='@itSHOWTIME how was the concert?', who=[]),\n",
       " Row(result=['Juda'], text=\"@audiobebop what time? I'm suppose to go to a concert tonight with Juda. uhh\", who=[]),\n",
       " Row(result=['@MizzDania'], text='@MizzDania How was the concert?', who=[]),\n",
       " Row(result=['@S_C_ @SongzYuuup'], text='The A was poppin @S_C_ @SongzYuuup and jeezy held the concert down.. Back in tally 2 papers and 2 midterms due tues. # backtoboredom', who=[]),\n",
       " Row(result=[], text='@melodyxxx LOL u ladies go hard! Are u guys going the kid cudi concert on Saturday?', who=[]),\n",
       " Row(result=[\"I'm\", 'Chile', 'Haiti', 'USA'], text=\"Look, I'm sorry about Chile and Haiti, but for real, do we need another benefit concert? COME ON! What about people here, in the USA?\", who=[]),\n",
       " Row(result=['Fred', 'Radio One', 'PAJAM'], text='U r!  had to get Fred cause he was doing a Radio One concert yesterday!  I start in Nov...and u know I got u!  In fact I want PAJAM to d ...', who=[])]"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who.select('*').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.join(who, on='text', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+--------+-------------------+------------+\n",
      "|text                                                                                                                                        |user_id |t_dt               |who         |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+--------+-------------------+------------+\n",
      "|For free - add your event to our calendar! http://bit.ly/T3lbr #seattle #music #events #event #bands #concerts #calendar #blog              |80949495|2009-11-02 15:10:01|[]          |\n",
      "|Jason Mraz was terrific at Red Rocks Sat night.  It's a truly unique & amazing concert venue.  @http://twitpic.com/ilfxr                    |42220821|2009-09-21 15:27:57|[Jason Mraz]|\n",
      "|My Daughter&#39;s First Orchestra Concert | Opensource, Nonprofits ... http://bit.ly/1mcbz8                                                 |18518302|2009-11-18 17:20:16|[]          |\n",
      "|I have 2 tickets to The Sounds concert tomorrow night at the House of Blues. $30 for both. Let me know if you want them.                    |21316433|2009-09-14 20:09:00|[The Sounds]|\n",
      "|music news: Washington 'Ring' cycle ends with riveting concert http://ow.ly/1621Lr                                                          |12135162|2009-11-16 08:37:38|[]          |\n",
      "|Headed over to Billy Bobs for the Diva Concert with Soap Nation.  If you are in the area... stop on by!!                                    |38184021|2009-08-22 11:01:16|[]          |\n",
      "|Zach Deputy = Amazing. Love just pours outta this guy... From an old concert --> http://htxt.it/aCWP                                        |13445142|2009-08-29 10:47:30|[]          |\n",
      "|@Stony419 Whoops missed your tweet last night. Warren G Concert. It was hilarious!                                                          |18991162|2009-11-17 10:42:51|[]          |\n",
      "|@q100Brittany OMG if i win tickets it would be the best day of my life its my dream to see Miley live at a concert & meet her  :)           |73886845|2009-11-13 13:10:54|[]          |\n",
      "|*dizzy* RT @jeskeets: MTV erects wall to block view of U2's free concert to commemorate the fall of the Berlin wall. ‚Äî http://bit.ly/XwLxt  |17494046|2009-11-05 21:56:26|[]          |\n",
      "|Fri Nov 27 Dudamel Conducts Salonen & Adams @ Walt Disney Concert Hall Los Angeles http://tinyurl.com/ykk3rt8                               |23047566|2009-11-19 10:20:04|[]          |\n",
      "|Open Platform #LiveNation?: artists/fans can upload own concerts, wikis, ratings, reviews, Twitter, Facebook +more: http://bit.ly/3UAIcI    |75577169|2009-10-29 09:46:51|[]          |\n",
      "|I can't wait to see NKOTB in concert @LiveNation with my $100 concert cash!!http://bit.ly/cT4csz                                            |26809809|2010-01-28 14:18:30|[NKOTB]     |\n",
      "|Dan Auerbach concert review: 11/17 at the Variety Playhouse http://www.waronpop.com                                                         |65071205|2009-11-18 14:49:56|[]          |\n",
      "|Don't miss Star Wars in Concert at the Rose Garden this Wednesday.  We still have rooms available! http://tiny.cc/WDsmT                     |26039487|2009-10-12 13:01:51|[]          |\n",
      "|@itssdanielaa haha nice datesss.;D and oh mann remember how we ate that whole box of cheese its after the mitchel concert! Hahaha           |23403892|2009-09-19 20:29:38|[]          |\n",
      "|Tonight! The Roots Concert & Afterparty l HOB l Doors at 8pm l Show at 9:15pm l Afterparty directly following the show in the Cambridge Room|33565318|2009-06-30 13:38:43|[]          |\n",
      "|Leaving for Bobby Long concert in an hour!!                                                                                                 |31333741|2009-11-05 16:38:30|[]          |\n",
      "|Last night I saw a ‚ÄúBarenaked Ladies‚Äù concert.  Total ripoff -- it was just fat ugly women on stage                                         |58601997|2009-10-18 13:25:09|[]          |\n",
      "|We are selling grupo unique tickets for there live concert here in chicago to get yours now call 773-807-6969 dont wait there going quickly!|33258398|2009-08-14 00:02:34|[]          |\n",
      "|Jay-z  Blueprint 3 Concert Tickets On Sale Now!: AUSTIN, TEXAS, November 22nd, Frank Erwin Centre - > http://bit.ly/4bkXjJ                  |47065812|2009-10-28 15:01:13|[]          |\n",
      "|@DustinLuminate hey, when ya'll do concerts do u take your own lighting & sound with you?                                                   |23347429|2009-08-19 22:49:53|[]          |\n",
      "|Right on man! So stoked she's back :) RT @TheophilusL: I wish I can go to a SADE concert all day where everyone just got stoned and made out|36264892|2009-12-25 10:41:40|[]          |\n",
      "|@MrPhifer u going to the concert fam?                                                                                                       |23710147|2009-11-09 17:12:33|[]          |\n",
      "|Man am I excited about Shock G of Digital Underground performing a 1 hour private concert at Affiliate Bash @ #affcon L.A.                  |22053527|2009-11-19 19:02:12|[]          |\n",
      "|Want 2C @songzyuuup (trey songz)& @mariosoultruth (mario) live in concert @ HOB hollywood on 9/29?! RT & Follow us 4 a shot to win!         |17659227|2009-09-23 13:53:25|[]          |\n",
      "|In Hawaii Tonight and Tomorrow  Screening of En Concert, then Jack Johnson and Zach Gill , to benefit Kokua. Tix at  www.hawaiitheatre.com  |19210865|2009-11-13 15:28:43|[]          |\n",
      "|is headed to the rock, is inspired by Brooke Fraser's concert on friday night, and encourages you to check out www.worldvision.org!         |15192038|2008-09-14 10:02:49|[]          |\n",
      "|Be sure and send the Murfreesboro Pulse all of your concert listings and events for next month soon - listings@boropulse.com                |80976799|2009-10-24 08:48:19|[]          |\n",
      "|i really hope i will get to meet @justinbieber at the radio disney concert in orlando!                                                      |28917642|2009-11-17 19:57:21|[]          |\n",
      "|The MJ tribute was good-except for Usher. Why was he trying to make it his concert?! He was showing off and I don't think he was genuine.   |55401665|2010-01-31 21:16:34|[Usher]     |\n",
      "|Final day of vacation is today.  No concert tonight; Cards game at Busch Stadium instead.                                                   |14722589|2009-09-15 12:09:00|[]          |\n",
      "|@st_vincent Your blogotheque concert with Andrew Bird absolutely blew my socks off http://bit.ly/4lTUpX I'm in love.                        |14856008|2009-10-22 01:25:54|[]          |\n",
      "|I'm so stoked for this concert. I'm a concert virgin.                                                                                       |34002860|2009-06-12 18:46:07|[]          |\n",
      "|1. i can NOT wait for the new @theaudition album and 2. i'm in dire need of a concert!                                                      |23559254|2010-03-12 15:53:13|[]          |\n",
      "|@jtshrinersopen Will there be a benefit concert in Vegas on the 17th again?!?!                                                              |57567704|2009-08-03 01:09:13|[]          |\n",
      "|First goal of this year accomplished went to aventura concert next goal hmmm its pending.....                                               |27231729|2010-02-01 00:00:00|[]          |\n",
      "|@mrslasky happy birthday enjoy the Garth Brooks concert for your big day!                                                                   |27013990|2009-11-29 08:40:39|[]          |\n",
      "|LIVE WEBCAM: Mike Perry and the Long Beds currently playing at the Back Stage Concert Series in downtown EC. Watch: http://volumeone.org    |18149671|2009-12-03 20:35:01|[]          |\n",
      "|YouTube to Live Stream Alicia Keys Concert Worldwide [VIDEO]: In October, YouTube hosted one of the biggest live st... http://bit.ly/6LhRnX |34171329|2009-11-26 19:23:39|[]          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+--------+-------------------+------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHEN: looking for date-related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_datetime download started this may take some time.\n",
      "Approx size to download 12.8 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# I'm going to start with the date matcher pretrained pipeline\n",
    "\n",
    "date_pipe = PretrainedPipeline(\"match_datetime\", lang=\"en\")\n",
    "\n",
    "date_annotation = date_pipe.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'user_id', 't_dt', 'who', 'document', 'sentence', 'token', 'date']"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_annotation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+\n",
      "|text                                                                                                                                        |t_dt               |result      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+\n",
      "|For free - add your event to our calendar! http://bit.ly/T3lbr #seattle #music #events #event #bands #concerts #calendar #blog              |2009-11-02 15:10:01|[]          |\n",
      "|Jason Mraz was terrific at Red Rocks Sat night.  It's a truly unique & amazing concert venue.  @http://twitpic.com/ilfxr                    |2009-09-21 15:27:57|[]          |\n",
      "|My Daughter&#39;s First Orchestra Concert | Opensource, Nonprofits ... http://bit.ly/1mcbz8                                                 |2009-11-18 17:20:16|[2020/09/08]|\n",
      "|I have 2 tickets to The Sounds concert tomorrow night at the House of Blues. $30 for both. Let me know if you want them.                    |2009-09-14 20:09:00|[2020/05/20]|\n",
      "|music news: Washington 'Ring' cycle ends with riveting concert http://ow.ly/1621Lr                                                          |2009-11-16 08:37:38|[]          |\n",
      "|Headed over to Billy Bobs for the Diva Concert with Soap Nation.  If you are in the area... stop on by!!                                    |2009-08-22 11:01:16|[]          |\n",
      "|Zach Deputy = Amazing. Love just pours outta this guy... From an old concert --> http://htxt.it/aCWP                                        |2009-08-29 10:47:30|[]          |\n",
      "|@Stony419 Whoops missed your tweet last night. Warren G Concert. It was hilarious!                                                          |2009-11-17 10:42:51|[]          |\n",
      "|@q100Brittany OMG if i win tickets it would be the best day of my life its my dream to see Miley live at a concert & meet her  :)           |2009-11-13 13:10:54|[]          |\n",
      "|*dizzy* RT @jeskeets: MTV erects wall to block view of U2's free concert to commemorate the fall of the Berlin wall. ‚Äî http://bit.ly/XwLxt  |2009-11-05 21:56:26|[]          |\n",
      "|Fri Nov 27 Dudamel Conducts Salonen & Adams @ Walt Disney Concert Hall Los Angeles http://tinyurl.com/ykk3rt8                               |2009-11-19 10:20:04|[2020/11/27]|\n",
      "|Open Platform #LiveNation?: artists/fans can upload own concerts, wikis, ratings, reviews, Twitter, Facebook +more: http://bit.ly/3UAIcI    |2009-10-29 09:46:51|[]          |\n",
      "|I can't wait to see NKOTB in concert @LiveNation with my $100 concert cash!!http://bit.ly/cT4csz                                            |2010-01-28 14:18:30|[]          |\n",
      "|Dan Auerbach concert review: 11/17 at the Variety Playhouse http://www.waronpop.com                                                         |2009-11-18 14:49:56|[]          |\n",
      "|Don't miss Star Wars in Concert at the Rose Garden this Wednesday.  We still have rooms available! http://tiny.cc/WDsmT                     |2009-10-12 13:01:51|[]          |\n",
      "|@itssdanielaa haha nice datesss.;D and oh mann remember how we ate that whole box of cheese its after the mitchel concert! Hahaha           |2009-09-19 20:29:38|[]          |\n",
      "|Tonight! The Roots Concert & Afterparty l HOB l Doors at 8pm l Show at 9:15pm l Afterparty directly following the show in the Cambridge Room|2009-06-30 13:38:43|[2020/05/19]|\n",
      "|Leaving for Bobby Long concert in an hour!!                                                                                                 |2009-11-05 16:38:30|[]          |\n",
      "|Last night I saw a ‚ÄúBarenaked Ladies‚Äù concert.  Total ripoff -- it was just fat ugly women on stage                                         |2009-10-18 13:25:09|[]          |\n",
      "|We are selling grupo unique tickets for there live concert here in chicago to get yours now call 773-807-6969 dont wait there going quickly!|2009-08-14 00:02:34|[]          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_annotation.select('text', 't_dt', 'date.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cool! It is using day-oriented words, like yesterday! I wonder if there is a way to set a reference date (as opposed to today). At least for the \"Radio One concert\" tweet... Doesn't look like there is, but I can use the date it outputs, get their relation with today, and apply to the date.\n",
    "\n",
    "I'm not sure how it got 12/06 from the \"Decemberists concert tonight\" tweet. - maybe december + the 6 hours later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_annotation = date_annotation.select('text', F.col('date.result').alias('date_result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/09/08']),\n",
       " Row(date_result=['2020/05/20']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/11/27']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/11/03']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=['2020/03/09']),\n",
       " Row(date_result=['2020/05/20']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/06/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/06/05']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2010/02/01']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/20']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/11/13']),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/12/06']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/18']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/08']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=['2020/05/19']),\n",
       " Row(date_result=['2020/01/07']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/10/10']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/20']),\n",
       " Row(date_result=['2020/09/13']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/06/20']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/11/14']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=['2020/05/18']),\n",
       " Row(date_result=['2020/04/01']),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[]),\n",
       " Row(date_result=[])]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_annotation.select('date_result').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'date_result']"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_annotation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- date_result: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_annotation.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(no_of_dates)|\n",
      "+----------------+\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_annotation.select(F.size(\"date_result\").alias(\"no_of_dates\")).agg({\"no_of_dates\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm deciding to take the first date, since in my small sample, no tweet had more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_annotation = date_annotation.withColumn('date_result', F.col('date_result')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|date_result|\n",
      "+-----------+\n",
      "|       null|\n",
      "|       null|\n",
      "| 2020/09/08|\n",
      "| 2020/05/20|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_annotation.select('date_result').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.join(date_annotation, on='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- t_dt: timestamp (nullable = true)\n",
      " |-- who: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- date_result: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.withColumn('date_result', F.to_date(sample['date_result'],'yyyy/MM/dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.withColumn('date_diff', F.datediff(F.current_timestamp(), sample['date_result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if date_result is within two weeks of today, get difference, and apply it to timestamp\n",
    "# elif date_result has this year's date. reset the year to match the year of the tweet \n",
    "# (hardcoeded as 10 years)\n",
    "\n",
    "sample = sample.withColumn('when', F.when((col('date_diff') > -14),\n",
    "                                      F.expr(\"date_add(t_dt, date_diff)\"))\\\n",
    "                          .when((F.col('date_diff') < -14) \n",
    "                                & (F.year('date_result') == F.year(F.current_timestamp())), \n",
    "                                F.date_sub('date_result', 3652))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------------+------------+-----------+---------+----------+\n",
      "|                text| user_id|               t_dt|         who|date_result|date_diff|      when|\n",
      "+--------------------+--------+-------------------+------------+-----------+---------+----------+\n",
      "|For free - add yo...|80949495|2009-11-02 15:10:01|          []|       null|     null|      null|\n",
      "|Jason Mraz was te...|42220821|2009-09-21 15:27:57|[Jason Mraz]|       null|     null|      null|\n",
      "|My Daughter&#39;s...|18518302|2009-11-18 17:20:16|          []| 2020-09-08|     -112|2010-09-09|\n",
      "|I have 2 tickets ...|21316433|2009-09-14 20:09:00|[The Sounds]| 2020-05-20|       -1|2009-09-13|\n",
      "+--------------------+--------+-------------------+------------+-----------+---------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.select('*').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop('date_result', 'date_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------------+------------+----------+\n",
      "|                text| user_id|               t_dt|         who|      when|\n",
      "+--------------------+--------+-------------------+------------+----------+\n",
      "|For free - add yo...|80949495|2009-11-02 15:10:01|          []|      null|\n",
      "|Jason Mraz was te...|42220821|2009-09-21 15:27:57|[Jason Mraz]|      null|\n",
      "|My Daughter&#39;s...|18518302|2009-11-18 17:20:16|          []|2010-09-09|\n",
      "|I have 2 tickets ...|21316433|2009-09-14 20:09:00|[The Sounds]|2009-09-13|\n",
      "|music news: Washi...|12135162|2009-11-16 08:37:38|          []|      null|\n",
      "|Headed over to Bi...|38184021|2009-08-22 11:01:16|          []|      null|\n",
      "|Zach Deputy = Ama...|13445142|2009-08-29 10:47:30|          []|      null|\n",
      "|@Stony419 Whoops ...|18991162|2009-11-17 10:42:51|          []|      null|\n",
      "|@q100Brittany OMG...|73886845|2009-11-13 13:10:54|          []|      null|\n",
      "|*dizzy* RT @jeske...|17494046|2009-11-05 21:56:26|          []|      null|\n",
      "|Fri Nov 27 Dudame...|23047566|2009-11-19 10:20:04|          []|2010-11-28|\n",
      "|Open Platform #Li...|75577169|2009-10-29 09:46:51|          []|      null|\n",
      "|I can't wait to s...|26809809|2010-01-28 14:18:30|     [NKOTB]|      null|\n",
      "|Dan Auerbach conc...|65071205|2009-11-18 14:49:56|          []|      null|\n",
      "|Don't miss Star W...|26039487|2009-10-12 13:01:51|          []|      null|\n",
      "|@itssdanielaa hah...|23403892|2009-09-19 20:29:38|          []|      null|\n",
      "|Tonight! The Root...|33565318|2009-06-30 13:38:43|          []|2009-06-30|\n",
      "|Leaving for Bobby...|31333741|2009-11-05 16:38:30|          []|      null|\n",
      "|Last night I saw ...|58601997|2009-10-18 13:25:09|          []|      null|\n",
      "|We are selling gr...|33258398|2009-08-14 00:02:34|          []|      null|\n",
      "+--------------------+--------+-------------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future: update \"when\" to have a non-hard-coded version of setting the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)\n",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
